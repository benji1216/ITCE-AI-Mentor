import streamlit as st
from google import genai
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# --- 1. é…ç½®æ–°ç‰ˆ Gemini Client ---
client = genai.Client(api_key=st.secrets["GEMINI_API_KEY"])

# --- 2. è¼‰å…¥çŸ¥è­˜åº« (ä½¿ç”¨å¿«å–é¿å…é‡è¤‡è®€å–) ---
@st.cache_resource
def load_itce_brain():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    # è¼‰å…¥ä½ å‰›æ‰ç”Ÿæˆçš„ faiss_itce_index è³‡æ–™å¤¾
    db = FAISS.load_local("faiss_itce_index", embeddings, allow_dangerous_deserialization=True)
    return db

# --- 3. ç¶²é  UI è¨­å®š ---
st.set_page_config(page_title="ITCE AI Mentor v2.5", page_icon="ğŸ›¡ï¸")
st.title("ğŸ›¡ï¸ ITCE åœ‹è²¿å¤§æœƒè€ƒ AI å°å¸« ")
st.caption("ğŸš€ ç›®å‰é‹è¡Œæ–¼ Gemini 2.5 Flash å¼•æ“")

try:
    db = load_itce_brain()
    
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # é¡¯ç¤ºèŠå¤©ç´€éŒ„
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("è©¢å•åœ‹è²¿è€ƒé»ï¼ˆä¾‹å¦‚ï¼šIncoterms ä¸­ D çµ„èˆ‡ C çµ„çš„å·®ç•°ï¼Ÿï¼‰"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨æª¢ç´¢è³‡æ–™åº«èˆ‡ç”Ÿæˆè§£æ..."):
                # RAG æª¢ç´¢ï¼šæ‰¾å‡ºæœ€ç›¸é—œçš„ 3 å€‹ç‰‡æ®µ
                docs = db.similarity_search(prompt, k=3)
                # çµ„åˆ context åŒæ™‚ç´€éŒ„ä¾†æº
                context_items = []
                for i, d in enumerate(docs):
                    page_num = d.metadata.get('page', 'æœªçŸ¥')
                    context_items.append(f"[ä¾†æº {i+1} - é ç¢¼ {page_num}]: {d.page_content}")

                context = "\n\n".join(context_items)

                # å»ºç«‹çµæ§‹åŒ–æŒ‡ä»¤
                response = client.models.generate_content(
                    model="gemini-2.5-flash", # âœ… æ”¹ç”¨ä½ æ¸…å–®ä¸­æœ‰çš„æœ€æ–°æ¨¡å‹
                    contents=f"ã€åƒè€ƒè³‡æ–™ã€‘\n{context}\n\nã€å•é¡Œã€‘\n{prompt}",
                    config={
                        'system_instruction': (
                            "ä½ æ˜¯ä¸€ä½å…·å‚™ 20 å¹´ç¶“é©—çš„åœ‹è²¿å¤§å¸«ï¼Œè«‹æ ¹æ“šåƒè€ƒè³‡æ–™æä¾›å°ˆæ¥­ã€æº–ç¢ºçš„ ITCE è€ƒè©¦è§£æã€‚"
                            "ã€å…§å®¹å“è³ªæª¢æŸ¥æ©Ÿåˆ¶ã€‘"
                            "   1. é›¶éºæ¼åŸå‰‡ï¼šæƒæ PDF è©²ç« ç¯€æ‰€æœ‰å…§å®¹ï¼ŒåŒ…å«è¡¨æ ¼ã€èªªæ˜æ–‡å­—ã€è€ƒå¤é¡Œã€‚åš´ç¦ç‚ºäº†ç¯€çœå­—æ•¸è€Œé€²è¡Œã€Œæ¦‚æ‹¬å¼ç¸½çµã€ï¼Œå¿…é ˆä¿ç•™æ‰€æœ‰ç´°ç¯€ã€‚" 
                            "   2. æ–°æ‰‹å°ç™½å‹å–„ï¼šé‡åˆ°ä»»ä½•å°ˆæ¥­ç¸®å¯« (ä¾‹å¦‚ï¼šC.C.C. Code, Forwarder, L/C)ï¼Œç¬¬ä¸€æ¬¡å‡ºç¾æ™‚å¿…é ˆç”¨ã€Œï¼ˆä¸­æ–‡åç¨± + ç™½è©±æ–‡è§£é‡‹ï¼‰ã€æ¨™è¨˜ã€‚"    
                            "   3. è€ƒå¤é¡Œå®Œæ•´æ€§ï¼šæ¯ä¸€é“è€ƒå¤é¡Œå¿…é ˆä¾åºåŒ…å«ï¼šã€Œå®Œæ•´é¡Œç›®ã€ + ã€Œ(A)(B)(C)(D)å››å€‹é¸é …å…§å®¹ã€ + ã€Œæ­£ç¢ºç­”æ¡ˆã€ + ã€Œè©³ç´°è§£æã€ã€‚åš´ç¦åªçµ¦è§£æä¸çµ¦é¡Œç›®æˆ–é¸é …ã€‚"    
                            "   4. é‚è¼¯é€£è²«ï¼šå…ˆè¬›ã€Œè§€å¿µã€ï¼Œå†è¬›ã€Œæµç¨‹ã€ï¼Œæœ€å¾Œæ¥ã€Œè€ƒå¤é¡Œå¯¦æˆ°ã€ï¼Œç¢ºä¿å­¸ç¿’éˆæ¢å®Œæ•´ã€‚"    
                            "   5. è¼¸å‡ºæœ€å¤§åŒ–ï¼šæœ€å¤§åŒ–è¼¸å‡ºå­—æ•¸ï¼Œç¢ºä¿æ‰€æœ‰å…§å®¹éƒ½è¢«è¬›è§£çš„è©³ç´°å®Œæ•´ "    
                            # "å›ç­”å¿…é ˆåŒ…å«ï¼š1.æ ¸å¿ƒæ¦‚å¿µ 2.æ³•è¦ä¾æ“š 3.æ­·å±†è€ƒé¡Œå¸¸è¦‹é™·é˜±ã€‚ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"
                        ),
                        'temperature': 0.2,
                    }
                )
                
                output_text = response.text
                st.markdown(output_text)
                st.session_state.messages.append({"role": "assistant", "content": output_text})

except Exception as e:
    st.error(f"âš ï¸ ç³»çµ±åˆå§‹åŒ–éŒ¯èª¤: {e}")